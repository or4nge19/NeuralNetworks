import Mathlib.Algebra.Lie.OfAssociative
import Mathlib.Analysis.Calculus.FDeriv.Comp
import Mathlib.Analysis.Calculus.Gradient.Basic
import Mathlib.Analysis.InnerProductSpace.Adjoint

open scoped InnerProductSpace RealInnerProductSpace

-- We work over a general Nontrivially Normed Field ğ•œ.
variable {ğ•œ : Type*} [NontriviallyNormedField ğ•œ]

namespace ContinuousLinearMap

variable {E F G : Type*}
  [NormedAddCommGroup E] [NormedSpace ğ•œ E]
  [NormedAddCommGroup F] [NormedSpace ğ•œ F]
  [NormedAddCommGroup G] [NormedSpace ğ•œ G]

/-- The continuous linear map that composes a continuous linear map with a given continuous linear
map `f` on the right. This is the "right-composition" operator.
`compRightL ğ•œ E F` is the map `g â†¦ g.comp f` where `f : E â†’L[ğ•œ] F` and `g : F â†’L[ğ•œ] G`. -/
noncomputable def compRightL (f : E â†’L[ğ•œ] F) : (F â†’L[ğ•œ] G) â†’L[ğ•œ] (E â†’L[ğ•œ] G) :=
  (ContinuousLinearMap.compL ğ•œ E F G).flip f

@[simp]
theorem compRightL_apply (f : E â†’L[ğ•œ] F) (g : F â†’L[ğ•œ] G) :
    compRightL f g = g.comp f :=
  rfl

/-- The dual map of a continuous linear map `f`, is the continuous linear map from the dual of the
codomain to the dual of the domain, given by pre-composition with `f`. -/
noncomputable def dualMap (f : E â†’L[ğ•œ] F) :
    StrongDual ğ•œ F â†’L[ğ•œ] StrongDual ğ•œ E :=
  compRightL f

@[simp]
theorem dualMap_apply {f : E â†’L[ğ•œ] F} {g : StrongDual ğ•œ F} :
    dualMap f g = g.comp f := rfl

@[simp]
theorem dualMap_apply_apply {f : E â†’L[ğ•œ] F} {g : StrongDual ğ•œ F} {x : E} :
    (dualMap f g) x = g (f x) := rfl

@[simp]
theorem dualMap_comp {f : E â†’L[ğ•œ] F} {g : F â†’L[ğ•œ] G} :
    dualMap (g.comp f) = (dualMap f).comp (dualMap g) := by
  ext h
  simp only [comp_apply, dualMap_apply, ContinuousLinearMap.comp_assoc]

end ContinuousLinearMap

/-!
# L1 Generalized: Differentiable Pullbacks (Banach Spaces)
-/

-- E, F, G are Normed Spaces over ğ•œ (Banach if CompleteSpace is assumed).
variable {E F G : Type*}
  [NormedAddCommGroup E] [NormedSpace ğ•œ E]
  [NormedAddCommGroup F] [NormedSpace ğ•œ F]
  [NormedAddCommGroup G] [NormedSpace ğ•œ G]

/--
The fundamental abstraction for differentiable computation in Banach spaces.
Represents a function and its backpropagator operating on the dual spaces (the pullback).
-/
structure DifferentiablePullback (E F : Type*) [NormedAddCommGroup E] [NormedSpace ğ•œ E]
  [NormedAddCommGroup F] [NormedSpace ğ•œ F] where
  view : E â†’ F
  h_diff : Differentiable ğ•œ view
  /-- The pullback: E â†’ (F* â†’L[ğ•œ] E*). Returns the dual map of Df(x). -/
  pullback : E â†’ (StrongDual ğ•œ F â†’L[ğ•œ] StrongDual ğ•œ E)
  /-- Correctness: The pullback map must be the dual map of the FrÃ©chet derivative. -/
  h_pullback : âˆ€ (x : E),
    pullback x = ContinuousLinearMap.dualMap (fderiv ğ•œ view x)

namespace DifferentiablePullback

def compose {E F G : Type*} [NormedAddCommGroup E] [NormedSpace ğ•œ E]
    [NormedAddCommGroup F] [NormedSpace ğ•œ F] [NormedAddCommGroup G] [NormedSpace ğ•œ G]
    (L1 : @DifferentiablePullback ğ•œ _ F G _ _ _ _) (L2 : @DifferentiablePullback ğ•œ _ E F _ _ _ _) :
    @DifferentiablePullback ğ•œ _ E G _ _ _ _ where
  view := L1.view âˆ˜ L2.view
  h_diff := L1.h_diff.comp L2.h_diff
  pullback := fun x =>
    -- (g âˆ˜ f)* = f* âˆ˜ g*
    (L2.pullback x).comp (L1.pullback (L2.view x))
  h_pullback := by
    intro x
    simp only [L2.h_pullback x, L1.h_pullback (L2.view x)]
    -- Now we have: goal is to show
    -- (L2.pullback x).comp (L1.pullback (L2.view x)) =
    -- dualMap (fderiv ğ•œ (L1.view âˆ˜ L2.view) x)
    rw [â† ContinuousLinearMap.dualMap_comp]
    -- Now we need to show:
    -- dualMap ((fderiv ğ•œ L1.view (L2.view x)).comp (fderiv ğ•œ L2.view x)) =
    -- dualMap (fderiv ğ•œ (L1.view âˆ˜ L2.view) x)
    congr 1
    rw [â† fderiv_comp x (L1.h_diff (L2.view x)) (L2.h_diff x)]

end DifferentiablePullback

/-!
# L1 Specialization: Differentiable Lenses (Hilbert Spaces)
-/

-- H1, H2, H3 are Hilbert spaces. We require ğ•œ' to be RCLike (â„ or â„‚) for the standard Hilbert adjoint.
variable {ğ•œ' : Type*} [RCLike ğ•œ']
variable {H1 H2 H3 : Type*}
  [NormedAddCommGroup H1] [InnerProductSpace ğ•œ' H1] [CompleteSpace H1]
  [NormedAddCommGroup H2] [InnerProductSpace ğ•œ' H2] [CompleteSpace H2]
  [NormedAddCommGroup H3] [InnerProductSpace ğ•œ' H3] [CompleteSpace H3]

/--
A Differentiable Lens in Hilbert spaces. Uses the Hilbert adjoint for gradient flow.
-/
structure DifferentiableLens (ğ•œ' : Type*) (H1 H2 : Type*)
  [RCLike ğ•œ']
  [NormedAddCommGroup H1] [InnerProductSpace ğ•œ' H1] [CompleteSpace H1]
  [NormedAddCommGroup H2] [InnerProductSpace ğ•œ' H2] [CompleteSpace H2] where
  view : H1 â†’ H2
  h_diff : Differentiable ğ•œ' view
  /-- The backward map (Adjoint): H1 â†’ (H2 â†’L[ğ•œ'] H1). -/
  update : H1 â†’ (H2 â†’L[ğ•œ'] H1)
  /-- Correctness: The update map must be the Hilbert adjoint of the FrÃ©chet derivative. -/
  h_update : âˆ€ (x : H1),
    update x = ContinuousLinearMap.adjoint (fderiv ğ•œ' view x)

namespace DifferentiableLens

/-- Composition of Lenses (The Chain Rule in Hilbert Spaces). -/
def compose {ğ•œ' : Type*} [RCLike ğ•œ'] {H1 H2 H3 : Type*}
    [NormedAddCommGroup H1] [InnerProductSpace ğ•œ' H1] [CompleteSpace H1]
    [NormedAddCommGroup H2] [InnerProductSpace ğ•œ' H2] [CompleteSpace H2]
    [NormedAddCommGroup H3] [InnerProductSpace ğ•œ' H3] [CompleteSpace H3]
    (L1 : DifferentiableLens ğ•œ' H1 H2) (L2 : DifferentiableLens ğ•œ' H2 H3) :
    DifferentiableLens ğ•œ' H1 H3 where
  view := L2.view âˆ˜ L1.view
  h_diff := L2.h_diff.comp L1.h_diff
  update := fun x =>
    let y := L1.view x
    -- (g âˆ˜ f)â€  = fâ€  âˆ˜ gâ€ 
    (L1.update x).comp (L2.update y)
  h_update := by
    intro x
    simp_rw [L1.h_update, L2.h_update (L1.view x)]
    -- Apply the chain rule: fderiv (g âˆ˜ f) x = (fderiv g (f x)) âˆ˜ (fderiv f x)
    rw [fderiv_comp x (L2.h_diff (L1.view x)) (L1.h_diff x)]
    rw [ContinuousLinearMap.adjoint_comp]

end DifferentiableLens

/-!
# L1 Refinement: Higher-Order Calculus
-/

section HigherOrderCalculus

variable {ğ•œ : Type*} [NontriviallyNormedField ğ•œ]
variable {E F : Type*} [NormedAddCommGroup E] [NormedSpace ğ•œ E] [NormedAddCommGroup F] [NormedSpace ğ•œ F]

/--
The second derivative (Hessian) of a function f: E â†’ F at x.
It is the FrÃ©chet derivative of the FrÃ©chet derivative map.
H(x) : E â†’L[ğ•œ] (E â†’L[ğ•œ] F).
-/
noncomputable def Hessian (f : E â†’ F) (x : E) : E â†’L[ğ•œ] (E â†’L[ğ•œ] F) :=
  fderiv (ğ•œ := ğ•œ) (fderiv (ğ•œ := ğ•œ) f) x

/--
Hessian-Vector Products (Hv-products). Computes (H(x)vâ‚vâ‚‚).
This is the second derivative evaluated in the directions vâ‚ and vâ‚‚.
-/
noncomputable def HessianVectorProduct (f : E â†’ F) (x vâ‚ vâ‚‚ : E) : F :=
  ((Hessian (ğ•œ := ğ•œ) (E := E) (F := F) f x) vâ‚) vâ‚‚

-- Note: Higher-order derivatives are accessed directly via `iteratedFDeriv ğ•œ n f x`.

-- H is a Hilbert space over â„ for optimization contexts (required for `gradient`).
variable {H : Type*} [NormedAddCommGroup H] [InnerProductSpace â„ H] [CompleteSpace H]

/--
Hessian-Vector Products (Hv-products) for scalar-valued functions. Computes H(x)v.
This utilizes the definition that the Hessian applied to v is the directional derivative
of the gradient along v (Forward-over-Reverse AD).
The result is a vector in H.
-/
noncomputable def HessianVectorProduct' (f : H â†’ â„) (x v : H) : H :=
  let g := gradient f
  (fderiv â„ g x) v

end HigherOrderCalculus

/-!
# L1 Refinement: The Riesz Bridge
We formalize the connection between the Banach dual map and the Hilbert adjoint.
-/

section RieszBridge

-- H1, H2 are Hilbert spaces over ğ•œ' (â„ or â„‚).
variable {ğ•œ' : Type*} [RCLike ğ•œ']
variable {H1 H2 : Type*}
  [NormedAddCommGroup H1] [InnerProductSpace ğ•œ' H1] [CompleteSpace H1]
  [NormedAddCommGroup H2] [InnerProductSpace ğ•œ' H2] [CompleteSpace H2]

/--
The Riesz Representation Map H â‰ƒL[ğ•œ'] H*.
It is a conjugate-linear isometric isomorphism targeting the `StrongDual`.
In mathlib: `InnerProductSpace.toDual`.
-/
noncomputable abbrev RieszMap (H : Type*) [NormedAddCommGroup H] [InnerProductSpace ğ•œ' H] [CompleteSpace H] :
  H â‰ƒâ‚—áµ¢â‹†[ğ•œ'] StrongDual ğ•œ' H :=
  InnerProductSpace.toDual ğ•œ' H

/--
Theorem: The Riesz Bridge.
The Hilbert adjoint Lâ€  is related to the Banach dual map L* by the Riesz isomorphisms R_H:
Lâ€  = Râ‚â»Â¹ âˆ˜ L* âˆ˜ Râ‚‚.
This shows that the optimization geometry (Hilbert adjoint) is derived from
the differentiation mechanism (Banach dual map).
-/
theorem riesz_bridge_adjoint
    (L : H1 â†’L[ğ•œ'] H2) :
    L.adjoint =
      ((RieszMap H1).symm.toContinuousLinearEquiv.toContinuousLinearMap).comp
        ((ContinuousLinearMap.dualMap L).comp
        ((RieszMap H2).toContinuousLinearEquiv.toContinuousLinearMap)) := by
  simp; exact rfl

end RieszBridge

/-!
# L2: Parameterized Lenses for Neural Networks
-/

section ParameterizedLens

variable {ğ•œ' : Type*} [RCLike ğ•œ']
variable {P H_in H_out : Type*}
  [NormedAddCommGroup P] [InnerProductSpace ğ•œ' P] [CompleteSpace P]
  [NormedAddCommGroup H_in] [InnerProductSpace ğ•œ' H_in] [CompleteSpace H_in]
  [NormedAddCommGroup H_out] [InnerProductSpace ğ•œ' H_out] [CompleteSpace H_out]

-- Provide an inner product space instance for the Unit type.
noncomputable instance : InnerProductSpace ğ•œ' Unit where
  inner _ _ := 0
  norm_sq_eq_re_inner _ := by simp
  conj_inner_symm _ _ := by simp
  add_left _ _ _ := by simp
  smul_left _ _ _ := by simp

/-
/--
A DifferentiableLens that is parameterized by a set of weights `P`.
The `view` function now takes parameters `p` and an input `x`.
The `update` function computes the adjoint, which can be used to derive gradients
with respect to both the parameters and the input.
-/
structure ParameterizedLens (ğ•œ' : Type*) (P H_in H_out : Type*)
  [RCLike ğ•œ']
  [NormedAddCommGroup P] [InnerProductSpace ğ•œ' P] [CompleteSpace P]
  [NormedAddCommGroup H_in] [InnerProductSpace ğ•œ' H_in] [CompleteSpace H_in]
  [NormedAddCommGroup H_out] [InnerProductSpace ğ•œ' H_out] [CompleteSpace H_out] where
  view : P â†’ H_in â†’ H_out
  h_diff : Differentiable ğ•œ' (fun (ph : P Ã— H_in) => view ph.1 ph.2)
  /-- The backward map (Adjoint): P Ã— H_in â†’ (H_out â†’L[ğ•œ'] P Ã— H_in). -/
  update : P â†’ H_in â†’ (H_out â†’L[ğ•œ'] P Ã— H_in)
  /-- Correctness: The update map must be the Hilbert adjoint of the FrÃ©chet derivative. -/
  h_update : âˆ€ (p : P) (x : H_in),
    update p x = ContinuousLinearMap.adjoint (fderiv ğ•œ' (fun (ph : P Ã— H_in) => view ph.1 ph.2) (p, x))

namespace ParameterizedLens

/--
An affine layer `x â†¦ Ax + b`.
Parameters `P` are `(H_in â†’L[ğ•œ'] H_out) Ã— H_out`, representing the matrix `A` and bias `b`.
We require `H_in` to be finite-dimensional to have an inner product on the space of linear maps.
-/
def affineLayer (H_in H_out : Type*) [NormedAddCommGroup H_in] [InnerProductSpace ğ•œ' H_in] [CompleteSpace H_in] [FiniteDimensional ğ•œ' H_in]
    [NormedAddCommGroup H_out] [InnerProductSpace ğ•œ' H_out] [CompleteSpace H_out] :
    ParameterizedLens ğ•œ' ((H_in â†’L[ğ•œ'] H_out) Ã— H_out) H_in H_out where
  view p x := p.1 x + p.2
  h_diff := by
    let f1 : ((H_in â†’L[ğ•œ'] H_out) Ã— H_out) Ã— H_in â†’ (H_in â†’L[ğ•œ'] H_out) Ã— H_in := fun p_x => (p_x.1.1, p_x.2)
    let f2 : (H_in â†’L[ğ•œ'] H_out) Ã— H_in â†’ H_out := fun p_x => p_x.1 p_x.2
    let f3 : ((H_in â†’L[ğ•œ'] H_out) Ã— H_out) Ã— H_in â†’ H_out := fun p_x => p_x.1.2
    have h_f1 : Differentiable ğ•œ' f1 := by simp; exact differentiable_fst.prod differentiable_snd
    have h_f2 : Differentiable ğ•œ' f2 := isBoundedBilinearMap_apply.differentiable
    have h_f3 : Differentiable ğ•œ' f3 := by simp; exact differentiable_snd.comp differentiable_fst
    exact (h_f2.comp h_f1).add h_f3
  update p x := ContinuousLinearMap.adjoint (fderiv ğ•œ' (fun ph => view ph.1 ph.2) (p, x))
  h_update p x := rfl

/--
An element-wise activation layer, e.g., ReLU or sigmoid.
It has no parameters (`P = Unit`), so it's a special case of a `ParameterizedLens`.
-/
def elementwise (f : H_in â†’ H_out) (h_f : Differentiable ğ•œ' f) :
    ParameterizedLens ğ•œ' Unit H_in H_out where
  view _ x := f x
  h_diff := Differentiable.comp h_f differentiable_snd
  update _ x := ContinuousLinearMap.adjoint (fderiv ğ•œ' (fun (ph : Unit Ã— H_in) => f ph.2) ((), x))
  h_update _ _ := rfl

/--
Mean Squared Error loss function: `L(y_pred, y_true) = â€–y_pred - y_trueâ€–Â²`.
This is a `ParameterizedLens` with `H_in = H_out Ã— H_out` (predicted and true values)
and output `H_out = â„`. It has no parameters (`P = Unit`).
-/
def mseLoss (H : Type*) [NormedAddCommGroup H] [InnerProductSpace ğ•œ' H] [CompleteSpace H] :
    ParameterizedLens ğ•œ' Unit (H Ã— H) â„ where
  view _ y_yh := â€–y_yh.1 - y_yh.2â€– ^ 2
  h_diff := by
    have h_norm_sq : Differentiable ğ•œ' (fun (v : H) => â€–vâ€– ^ 2) := by
      simp_rw [â† inner_self_eq_norm_sq_to_K]
      have := isBoundedBilinearMap_inner ğ•œ' H
      exact this.differentiable.comp (differentiable_id.prod_mk differentiable_id)
    exact Differentiable.comp h_norm_sq (differentiable_fst.sub differentiable_snd)
  update _ y_yh := ContinuousLinearMap.adjoint (fderiv ğ•œ' (fun (ph : Unit Ã— (H Ã— H)) => â€–ph.2.1 - ph.2.2â€– ^ 2) ((), y_yh))
  h_update _ _ := rfl

variable {P1 H1 H2 P2 H3 : Type*}
  [NormedAddCommGroup P1] [InnerProductSpace ğ•œ' P1] [CompleteSpace P1]
  [NormedAddCommGroup H1] [InnerProductSpace ğ•œ' H1] [CompleteSpace H1]
  [NormedAddCommGroup H2] [InnerProductSpace ğ•œ' H2] [CompleteSpace H2]
  [NormedAddCommGroup P2] [InnerProductSpace ğ•œ' P2] [CompleteSpace P2]
  [NormedAddCommGroup H3] [InnerProductSpace ğ•œ' H3] [CompleteSpace H3]

/-- Composition of ParameterizedLenses. -/
def compose (L2 : ParameterizedLens ğ•œ' P2 H2 H3) (L1 : ParameterizedLens ğ•œ' P1 H1 H2) :
    ParameterizedLens ğ•œ' (P1 Ã— P2) H1 H3 where
  view p x := L2.view p.2 (L1.view p.1 x)
  h_diff := by
    let f_combined : (P1 Ã— P2) Ã— H1 â†’ (P2 Ã— H2) :=
      fun p_x => (p_x.1.2, L1.view p_x.1.1 p_x.2)
    have h_f_combined : Differentiable ğ•œ' f_combined :=
      (differentiable_snd.comp differentiable_fst).prod (L1.h_diff.comp ((differentiable_fst.comp differentiable_fst).prod differentiable_snd))
    exact L2.h_diff.comp h_f_combined
  update p x := ContinuousLinearMap.adjoint (fderiv ğ•œ' (fun ph => view ph.1 ph.2) (p, x))
  h_update p x := rfl

/--
A single step of gradient descent for a parameterized lens.
Computes the gradient of the loss with respect to the parameters and updates them.
-/
def gradientDescentStep
    (L : ParameterizedLens ğ•œ' P H_in H_out)
    (loss : ParameterizedLens ğ•œ' Unit (H_out Ã— H_out) â„)
    (p : P) (x : H_in) (y_true : H_out) (Î· : â„) : P :=
  let y_pred := L.view p x
  let _loss_val := loss.view () (y_pred, y_true)
  -- The adjoint of the composed forward map gives the gradients.
  -- The derivative of the loss w.r.t. its input is needed.
  -- For MSE `â€–y - y'â€–Â²`, the gradient w.r.t. `y` is `2(y - y')`.
  -- The `update` function for the loss gives the adjoint of the derivative.
  -- We apply it to `1` (the gradient of the identity function `z â†¦ z` at `loss_val`).
  let dL_dy_pred_adj : Unit Ã— (H_out Ã— H_out) := (loss.update () (y_pred, y_true)) (1 : â„)
  -- The adjoint returns a pair of gradients: (w.r.t. y_pred, w.r.t. y_true). We need the first.
  let dL_dy_pred := dL_dy_pred_adj.2.1
  -- Propagate this gradient back through the lens L.
  let grads := (L.update p x) dL_dy_pred
  -- The result is a pair of gradients: (w.r.t. parameters, w.r.t. input). We need the first.
  let dL_dp := grads.1
  -- Update parameters
  p - (Î· : ğ•œ') â€¢ dL_dp

end ParameterizedLens
-/

/-!
# Differentiable computational blocks and VJPs (general Banach + Hilbert specializations)

A CompBlock is a smooth, parameterized operation fwd : P Ã— X â†’ Y over a base field ğ•œ.
We expose its Jacobian (via fderiv), a Banach-space VJP (via the dual map), and a
Hilbert-space VJP (via the adjoint). We also provide a bridge back into your
DifferentiablePullback abstraction for reuse of composition theorems.
-/

section CompBlock

variable {ğ•œ : Type*} [NontriviallyNormedField ğ•œ]
variable {P X Y : Type*}
  [NormedAddCommGroup P] [NormedSpace ğ•œ P]
  [NormedAddCommGroup X] [NormedSpace ğ•œ X]
  [NormedAddCommGroup Y] [NormedSpace ğ•œ Y]

/-- A differentiable, parameterized block: fwd : P Ã— X â†’ Y with a differentiability certificate. -/
structure CompBlock (ğ•œ : Type*) [NontriviallyNormedField ğ•œ]
    (P X Y : Type*)
    [NormedAddCommGroup P] [NormedSpace ğ•œ P]
    [NormedAddCommGroup X] [NormedSpace ğ•œ X]
    [NormedAddCommGroup Y] [NormedSpace ğ•œ Y] where
  fwd   : P Ã— X â†’ Y
  diff  : Differentiable ğ•œ fwd

namespace CompBlock

/-- The FrÃ©chet derivative (Jacobian) of a block at (p, x). -/
noncomputable def jacobian
    (B : CompBlock ğ•œ P X Y) (p : P) (x : X) : (P Ã— X) â†’L[ğ•œ] Y :=
  fderiv ğ•œ B.fwd (p, x)

/-- Banach VJP: the pullback on duals (StrongDual) induced by the Jacobian. -/
noncomputable def vjpBanach
    (B : CompBlock ğ•œ P X Y) (p : P) (x : X) :
    (StrongDual ğ•œ Y) â†’L[ğ•œ] (StrongDual ğ•œ (P Ã— X)) :=
  ContinuousLinearMap.dualMap (B.jacobian p x)

/-- Package the block as a DifferentiablePullback from P Ã— X to Y. -/
noncomputable def toDifferentiablePullback
    (B : CompBlock ğ•œ P X Y) :
    @DifferentiablePullback ğ•œ _ (P Ã— X) Y _ _ _ _ where
  view := B.fwd
  h_diff := B.diff
  pullback := fun z => ContinuousLinearMap.dualMap (fderiv ğ•œ B.fwd z)
  h_pullback := by intro z; rfl

/- Hilbert-space VJP via the adjoint (requires inner products over an RCLike field). -/
variable {ğ•œ' : Type*} [RCLike ğ•œ']
variable {P' X' Y' : Type*}
  [NormedAddCommGroup P'] [InnerProductSpace ğ•œ' P'] [CompleteSpace P']
  [NormedAddCommGroup X'] [InnerProductSpace ğ•œ' X'] [CompleteSpace X']
  [NormedAddCommGroup Y'] [InnerProductSpace ğ•œ' Y'] [CompleteSpace Y']

open scoped InnerProductSpace RealInnerProductSpace PiLp EuclideanSpace

/-
noncomputable def vjpHilbert
    (B : CompBlock ğ•œ' P' X' Y') (p : P') (x : X') :
    Y' â†’L[ğ•œ'] (P' Ã— X') :=
  have := Prod.innerProductSpace
  ContinuousLinearMap.adjoint (fderiv ğ•œ' B.fwd (p, x))
  -/

end CompBlock
end CompBlock
